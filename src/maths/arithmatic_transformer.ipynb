{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import required packages"
      ],
      "metadata": {
        "id": "7Wb4AbgTeVh7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "outputs": [],
      "source": [
        "import time\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import math\n",
        "import random\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning)"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-23T00:12:45.012058100Z",
          "start_time": "2025-03-23T00:12:43.191178500Z"
        },
        "id": "s8ciwqp4eRD3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "g_yDtzc7eeZ2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define Hyperparameters\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print('running on device:', device)\n",
        "batch_size = 5000\n",
        "epochs = 50\n",
        "learning_rate = 0.001\n",
        "MODEL_PATH = 'math_transformer.pth'"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QeypjX0RX1kO",
        "outputId": "f3fdaa3a-d5c9-461e-9a97-65bb65347133",
        "ExecuteTime": {
          "end_time": "2025-03-23T00:14:09.432510500Z",
          "start_time": "2025-03-23T00:12:45.054175200Z"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "running on device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define characters\n",
        "chars = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9',\n",
        "         '+', '-', '*', '/', '=', '<SOS>', '<EOS>', '<PAD>']\n",
        "vocab = {c: i for i, c in enumerate(chars)}\n",
        "vocab_size = len(vocab)\n",
        "SOS_token = vocab['<SOS>']\n",
        "EOS_token = vocab['<EOS>']\n",
        "PAD_token = vocab['<PAD>']"
      ],
      "metadata": {
        "id": "Le-IWkA4XqPV",
        "ExecuteTime": {
          "end_time": "2025-03-23T00:12:45.026547200Z",
          "start_time": "2025-03-23T00:12:45.019779Z"
        }
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [],
      "source": [
        "# Generate Mathematical Samples\n",
        "def generate_math_dataset(num_samples, max_digits=1, operations=('+', '-', '*', '/')):\n",
        "    samples = []\n",
        "    for _ in range(num_samples):\n",
        "        op = random.choice(operations)\n",
        "        if op in ['+', '-', '*']:\n",
        "            a = random.randint(10 ** (max_digits - 1), 10 ** max_digits - 1)\n",
        "            b = random.randint(10 ** (max_digits - 1), 10 ** max_digits - 1)\n",
        "            if op == '-' and a < b:\n",
        "                a, b = b, a\n",
        "            res = eval(f\"{a}{op}{b}\")\n",
        "        elif op == '/':\n",
        "            b = random.randint(10 ** (max_digits - 1), 10 ** max_digits - 1)\n",
        "            res = random.randint(10 ** (max_digits - 1), 10 ** max_digits - 1)\n",
        "            a = res * b\n",
        "        question = f\"{a}{op}{b}=\"\n",
        "        answer = f\"{res}\"\n",
        "        samples.append((question, answer))\n",
        "    return samples"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2025-03-23T00:37:43.933336100Z"
        },
        "id": "tdlQqOMVeRD4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [],
      "source": [
        "# Math Dataset Class\n",
        "class MathDataset(Dataset):\n",
        "    def __init__(self, samples, vocab, max_question_len=8, max_answer_len=6):\n",
        "        self.samples = samples\n",
        "        self.vocab = vocab\n",
        "        self.max_q = max_question_len\n",
        "        self.max_a = max_answer_len + 2  # Include SOS and EOS\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        q, a = self.samples[idx]\n",
        "        # Wrap questions\n",
        "        q_ids = [vocab[c] for c in q][:self.max_q]\n",
        "        q_pad = [PAD_token] * (self.max_q - len(q_ids))\n",
        "        q_ids += q_pad\n",
        "\n",
        "        # Wrap answers\n",
        "        a_ids = [SOS_token] + [vocab[c] for c in a][:self.max_a]\n",
        "        a_ids += [EOS_token]\n",
        "        a_ids = a_ids[:self.max_a]\n",
        "        a_pad = [PAD_token] * (self.max_a - len(a_ids))\n",
        "        a_ids += a_pad\n",
        "\n",
        "        return torch.LongTensor(q_ids), torch.LongTensor(a_ids)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2025-03-23T00:12:45.029547700Z"
        },
        "id": "DTY998x8eRD5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": [
        "# Positional Encoding\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
        "        super().__init__()\n",
        "        self.dropout = nn.Dropout(p=dropout)\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.register_buffer('pe', pe)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.pe[:x.size(1)]\n",
        "        return self.dropout(x)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2025-03-23T00:12:45.033995900Z"
        },
        "id": "iWpFEunoeRD5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [],
      "source": [
        "# Math Transformer Model\n",
        "class MathTransformer(nn.Module):\n",
        "    def __init__(self, vocab_size, d_model=128, nhead=4, num_layers=3, dim_feedforward=512, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.d_model = d_model\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=nhead,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=dim_feedforward,\n",
        "            dropout=dropout,\n",
        "            batch_first=False,\n",
        "        )\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt, src_mask=None, tgt_mask=None,\n",
        "                src_padding_mask=None, tgt_padding_mask=None):\n",
        "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
        "        src = self.pos_encoder(src)\n",
        "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
        "        tgt = self.pos_encoder(tgt)\n",
        "\n",
        "        output = self.transformer(\n",
        "            src, tgt,\n",
        "            src_mask=src_mask,\n",
        "            tgt_mask=tgt_mask,\n",
        "            src_key_padding_mask=src_padding_mask,\n",
        "            tgt_key_padding_mask=tgt_padding_mask,\n",
        "            memory_key_padding_mask=src_padding_mask\n",
        "        )\n",
        "        return self.fc_out(output)"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2025-03-23T00:12:45.042783500Z"
        },
        "id": "FHRccP5KeRD5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "outputs": [],
      "source": [
        "# Generate mask\n",
        "def generate_mask(src, tgt, PAD_token):\n",
        "    src_seq_len = src.shape[0]\n",
        "    tgt_seq_len = tgt.shape[0]\n",
        "\n",
        "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt_seq_len)\n",
        "    src_mask = torch.zeros((src_seq_len, src_seq_len)).bool()\n",
        "\n",
        "    src_padding_mask = (src == PAD_token).transpose(0, 1)\n",
        "    tgt_padding_mask = (tgt == PAD_token).transpose(0, 1)\n",
        "\n",
        "    return src_mask, tgt_mask, src_padding_mask, tgt_padding_mask"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2025-03-23T00:12:45.045805500Z"
        },
        "id": "Fz3kLRZLeRD5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [],
      "source": [
        "# Generate Dataset\n",
        "train_data = generate_math_dataset(20000)\n",
        "val_data = generate_math_dataset(1000)\n",
        "train_dataset = MathDataset(train_data, vocab)\n",
        "val_dataset = MathDataset(val_data, vocab)\n",
        "\n",
        "# Create Data Loader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "zYKBK6aIeRD5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 2.4317\n",
            "Epoch 11, Loss: 1.0089\n",
            "Epoch 21, Loss: 0.2543\n",
            "Epoch 31, Loss: 0.0735\n",
            "Epoch 41, Loss: 0.0485\n",
            "time elapsed: 91.75194668769836\n"
          ]
        }
      ],
      "source": [
        "# Initiate model\n",
        "model = MathTransformer(vocab_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_token)\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
        "t0 = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in train_loader:\n",
        "        src = src.transpose(0, 1).to(device)  # (seq_len, batch)\n",
        "        tgt = tgt.transpose(0, 1).to(device)\n",
        "\n",
        "        tgt_input = tgt[:-1, :]  # decoder input\n",
        "        tgt_output = tgt[1:, :]  # decoder target\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        src_mask, tgt_mask, src_pad_mask, tgt_pad_mask = generate_mask(src, tgt_input, PAD_token)\n",
        "        src_mask = src_mask.to(device)\n",
        "        tgt_mask = tgt_mask.to(device)\n",
        "        output = model(src, tgt_input, src_mask, tgt_mask, src_pad_mask, tgt_pad_mask)\n",
        "\n",
        "        loss = criterion(output.reshape(-1, output.shape[-1]), tgt_output.reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "    if epoch % 10 == 0:\n",
        "      print(f\"Epoch {epoch + 1}, Loss: {total_loss / len(train_loader):.4f}\")\n",
        "print('time elapsed:', time.time() - t0)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y1Va2G2seRD5",
        "outputId": "0419e810-c4c7-4070-f256-5ca7a19c810b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the trained model\n",
        "torch.save(model.state_dict(), MODEL_PATH)"
      ],
      "metadata": {
        "id": "7-oXCaHRZHPg",
        "ExecuteTime": {
          "end_time": "2025-03-23T00:14:09.449440400Z",
          "start_time": "2025-03-23T00:14:09.426098800Z"
        }
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "outputs": [],
      "source": [
        "# Training models in GPU and Predicting in CPU\n",
        "# loaded_model.load_state_dict(torch.load('math_transformer.pth', map_location=torch.device('cpu')))"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2025-03-23T00:14:09.450438700Z"
        },
        "id": "dfVcQwC2eRD5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "outputs": [],
      "source": [
        "# Load model\n",
        "loaded_model = MathTransformer(vocab_size).to(device)\n",
        "loaded_model.load_state_dict(torch.load(MODEL_PATH))\n",
        "loaded_model.eval();"
      ],
      "metadata": {
        "id": "t2BtOh7qeRD6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "outputs": [],
      "source": [
        "# Test the model\n",
        "def predict(model, question, vocab, max_length=10):\n",
        "    model.eval()\n",
        "    src = torch.LongTensor([vocab[c] for c in question]).unsqueeze(1).to(device)\n",
        "    memory = model.transformer.encoder(model.pos_encoder(model.embedding(src) * math.sqrt(model.d_model)))\n",
        "    ys = torch.LongTensor([[SOS_token]]).to(device)\n",
        "\n",
        "    for _ in range(max_length):\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(ys.size(0)).to(device)\n",
        "        out = model.transformer.decoder(model.pos_encoder(model.embedding(ys) * math.sqrt(model.d_model)),\n",
        "                                        memory, tgt_mask)\n",
        "        out = model.fc_out(out[-1, :])\n",
        "        next_token = out.argmax().item()\n",
        "        ys = torch.cat([ys, torch.LongTensor([[next_token]]).to(device)], dim=0)\n",
        "\n",
        "        if next_token == EOS_token:\n",
        "            break\n",
        "\n",
        "    return ''.join([chars[i] for i in ys.squeeze().tolist() if i not in [SOS_token, EOS_token, PAD_token]])"
      ],
      "metadata": {
        "ExecuteTime": {
          "start_time": "2025-03-23T00:14:09.458406800Z"
        },
        "id": "ZymHkHQ-eRD6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question: 7-1= Correct Answer: 6 Predicted Result: 6\n",
            "Question: 6+3= Correct Answer: 9 Predicted Result: 9\n",
            "Question: 9-2= Correct Answer: 7 Predicted Result: 7\n",
            "Question: 7-3= Correct Answer: 4 Predicted Result: 4\n",
            "Question: 3+5= Correct Answer: 8 Predicted Result: 8\n",
            "Question: 5+1= Correct Answer: 6 Predicted Result: 6\n",
            "Question: 6-5= Correct Answer: 1 Predicted Result: 1\n",
            "Question: 9*8= Correct Answer: 72 Predicted Result: 72\n",
            "Question: 4+7= Correct Answer: 11 Predicted Result: 11\n",
            "Question: 3+8= Correct Answer: 11 Predicted Result: 11\n",
            "Question: 16/8= Correct Answer: 2 Predicted Result: 3\n",
            "Question: 9+4= Correct Answer: 13 Predicted Result: 13\n",
            "Question: 7+7= Correct Answer: 14 Predicted Result: 14\n",
            "Question: 32/8= Correct Answer: 4 Predicted Result: 4\n",
            "Question: 10/2= Correct Answer: 5 Predicted Result: 5\n",
            "Question: 1*9= Correct Answer: 9 Predicted Result: 9\n",
            "Question: 9+1= Correct Answer: 10 Predicted Result: 10\n",
            "Question: 1/1= Correct Answer: 1 Predicted Result: 1\n",
            "Question: 8+3= Correct Answer: 11 Predicted Result: 11\n",
            "Question: 6/3= Correct Answer: 2 Predicted Result: 2\n"
          ]
        }
      ],
      "source": [
        "# Test the model\n",
        "test_samples = generate_math_dataset(20)\n",
        "for q, a in test_samples:\n",
        "    pred = predict(loaded_model, q, vocab)\n",
        "    print(f\"Question: {q} Correct Answer: {a} Predicted Result: {pred}\")\n"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2025-03-23T00:40:23.745921900Z",
          "start_time": "2025-03-23T00:40:23.267246800Z"
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "shq16575eRD6",
        "outputId": "0fa03e6e-6562-4298-8f2c-a556b53af5cd"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MathTransformer(\n",
              "  (embedding): Embedding(18, 128)\n",
              "  (pos_encoder): PositionalEncoding(\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (transformer): Transformer(\n",
              "    (encoder): TransformerEncoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-2): 3 x TransformerEncoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "    (decoder): TransformerDecoder(\n",
              "      (layers): ModuleList(\n",
              "        (0-2): 3 x TransformerDecoderLayer(\n",
              "          (self_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "          )\n",
              "          (multihead_attn): MultiheadAttention(\n",
              "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
              "          )\n",
              "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
              "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout1): Dropout(p=0.1, inplace=False)\n",
              "          (dropout2): Dropout(p=0.1, inplace=False)\n",
              "          (dropout3): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "      (norm): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
              "    )\n",
              "  )\n",
              "  (fc_out): Linear(in_features=128, out_features=18, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "loaded_model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E9YyOEhQeRD6",
        "outputId": "c8f4f7ae-4540-4cd9-e801-1c2549ec81ba"
      }
    }
  ]
}